{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4rAORtR9Rpn"
      },
      "outputs": [],
      "source": [
        "# Principais bibliotecas\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime, date, timedelta\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import NotFound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WMjW1F0-LKV"
      },
      "outputs": [],
      "source": [
        "# Autenticação no Google Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNZ-2UMe-cp2"
      },
      "outputs": [],
      "source": [
        "# Define as variáveis de ambiente\n",
        "API_KEY = '66fa5192ea29b6e667661468f424eb01' # Chave da API da API-Sports\n",
        "API_URL = 'https://v3.football.api-sports.io' # URL base da API da API-Sports\n",
        "PROJECT_ID = 'jopecasports' # ID do projeto no BigQuery\n",
        "DATASET_NAME = 'soccer_analysis' # Nome do dataset no BigQuery\n",
        "FULL_LOAD_DATE = '2023-01-01' # Data para carga completa inicial\n",
        "LEAGUE = 2 # ID da liga na API-Sports\n",
        "SEASON = 2023 # Ano da temporada\n",
        "\n",
        "# Define o cabeçalho da requisição para a API\n",
        "HEADERS = {\n",
        "    'x-rapidapi-key': API_KEY\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variáveis de ambiente\n",
        "API_KEY = 'Aqui deve ser colocada a chave da API'\n",
        "API_URL = 'https://v3.football.api-sports.io'\n",
        "PROJECT_ID = 'jopecasports'\n",
        "DATASET_NAME = 'soccer_analysis'\n",
        "FULL_LOAD_DATE = '2023-01-01'\n",
        "LEAGUE = 2\n",
        "SEASON = 2023\n",
        "\n",
        "# Cabeçalho da requisição para a API\n",
        "HEADERS = {\n",
        "    'x-rapidapi-key': API_KEY\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZp7k45eAChw"
      },
      "outputs": [],
      "source": [
        "# Define os endpoints da API a serem consumidos\n",
        "endpoints = [\n",
        "    {\n",
        "        \"table\": \"past_fixtures\", # Nome da tabela no BigQuery\n",
        "        \"write_disposition\": \"WRITE_APPEND\", # Define o método de escrita no BigQuery (append)\n",
        "        \"path\": \"fixtures\", # Caminho do endpoint na API-Sports\n",
        "        \"quality_control\": False, # Flag para controle de qualidade (não utilizado neste caso)\n",
        "        \"params\": { # Parâmetros da requisição para a API-Sports\n",
        "            \"league\": LEAGUE,\n",
        "            \"season\": SEASON\n",
        "        },\n",
        "        \"incremental_load_params\": { # Parâmetros para a carga incremental\n",
        "            \"from\": \"YYYY-MM-DD\", # Data de início para a carga incremental\n",
        "            \"to\": \"YYYY-MM-DD\" # Data de fim para a carga incremental\n",
        "        },\n",
        "        \"fields\": [], # Campos a serem selecionados na resposta da API (não utilizado neste caso)\n",
        "        \"nested_fields\": [ # Campos aninhados a serem extraídos da resposta da API\n",
        "            \"fixture.id\",\n",
        "            \"fixture.referee\",\n",
        "            \"fixture.timezone\",\n",
        "            \"fixture.date\",\n",
        "            \"fixture.timestamp\",\n",
        "            \"fixture.periods.first\",\n",
        "            \"fixture.periods.second\",\n",
        "            \"fixture.venue.id\",\n",
        "            \"fixture.venue.name\",\n",
        "            \"fixture.venue.city\",\n",
        "            \"fixture.status.long\",\n",
        "            \"fixture.status.short\",\n",
        "            \"fixture.status.elapsed\",\n",
        "            \"league.id\",\n",
        "            \"league.name\",\n",
        "            \"league.country\",\n",
        "            \"league.logo\",\n",
        "            \"league.flag\",\n",
        "            \"league.season\",\n",
        "            \"league.round\",\n",
        "            \"teams.home.id\",\n",
        "            \"teams.home.name\",\n",
        "            \"teams.home.logo\",\n",
        "            \"teams.home.winner\",\n",
        "            \"teams.away.id\",\n",
        "            \"teams.away.name\",\n",
        "            \"teams.away.logo\",\n",
        "            \"teams.away.winner\",\n",
        "            \"goals.home\",\n",
        "            \"goals.away\",\n",
        "            \"score.halftime.home\",\n",
        "            \"score.halftime.away\",\n",
        "            \"score.fulltime.home\",\n",
        "            \"score.fulltime.away\",\n",
        "            \"score.extratime.home\",\n",
        "            \"score.extratime.away\",\n",
        "            \"score.penalty.home\",\n",
        "            \"score.penalty.away\"\n",
        "        ],\n",
        "        \"repeatable_fields\": [] # Campos que se repetem na resposta da API (não utilizado neste caso)\n",
        "    },\n",
        "    # Próximos endpoints seguem a mesma estrutura do anterior\n",
        "    {\n",
        "        \"table\": \"future_fixtures\",\n",
        "        \"write_disposition\": \"WRITE_TRUNCATE\", # Define o método de escrita no BigQuery (truncate)\n",
        "        \"path\": \"fixtures\",\n",
        "        \"quality_control\": False,\n",
        "        \"params\": {\n",
        "            \"league\": LEAGUE,\n",
        "            \"season\": SEASON,\n",
        "            \"to\": \"2099-12-31\"\n",
        "        },\n",
        "        \"incremental_load_params\": {\n",
        "            \"from\": \"YYYY-MM-DD\",\n",
        "        },\n",
        "        \"fields\": [],\n",
        "        \"nested_fields\": [\n",
        "            \"fixture.id\",\n",
        "            \"fixture.timezone\",\n",
        "            \"fixture.date\",\n",
        "            \"fixture.timestamp\",\n",
        "            \"fixture.venue.id\",\n",
        "            \"fixture.venue.name\",\n",
        "            \"fixture.venue.city\",\n",
        "            \"fixture.status.long\",\n",
        "            \"fixture.status.short\",\n",
        "            \"league.id\",\n",
        "            \"league.name\",\n",
        "            \"league.country\",\n",
        "            \"league.logo\",\n",
        "            \"league.flag\",\n",
        "            \"league.season\",\n",
        "            \"league.round\",\n",
        "            \"teams.home.id\",\n",
        "            \"teams.home.name\",\n",
        "            \"teams.home.logo\",\n",
        "            \"teams.away.id\",\n",
        "            \"teams.away.name\",\n",
        "            \"teams.away.logo\"\n",
        "        ],\n",
        "        \"repeatable_fields\": []\n",
        "    },\n",
        "    {\n",
        "        \"table\": \"players\",\n",
        "        \"write_disposition\": \"WRITE_TRUNCATE\", # Define o método de escrita no BigQuery (truncate)\n",
        "        \"path\": \"players\",\n",
        "        \"quality_control\": True, # Flag para controle de qualidade (não utilizado neste caso)\n",
        "        \"params\": {\n",
        "            \"league\": LEAGUE,\n",
        "            \"season\": SEASON,\n",
        "            \"page\": 1 # Número da página de resultados da API\n",
        "        },\n",
        "        \"fields\": [],\n",
        "        \"nested_fields\": [\n",
        "            \"player.id\",\n",
        "            \"player.name\",\n",
        "            \"player.firstname\",\n",
        "            \"player.lastname\",\n",
        "            \"player.age\",\n",
        "            \"player.birth.date\",\n",
        "            \"player.birth.place\",\n",
        "            \"player.nationality\",\n",
        "            \"player.height\",\n",
        "            \"player.weight\",\n",
        "            \"player.injured\",\n",
        "            \"player.photo\"\n",
        "        ],\n",
        "        \"repeatable_fields\": []\n",
        "    },\n",
        "]\n",
        "\n",
        "# Define os endpoints da API a serem consumidos de forma iterativa,\n",
        "# ou seja, para cada item do endpoint principal, este endpoint será chamado\n",
        "iterable_endpoints = {\n",
        "    \"past_fixtures\": [ # Define os endpoints iteráveis para o endpoint \"past_fixtures\"\n",
        "        {\n",
        "            \"table\": \"fixturesStatistics\", # Nome da tabela no BigQuery\n",
        "            \"write_disposition\": \"WRITE_APPEND\", # Define o método de escrita no BigQuery (append)\n",
        "            \"path\": \"fixtures/statistics\", # Caminho do endpoint na API-Sports\n",
        "            \"query_param\": { # Define o parâmetro da requisição para a API-Sports que será usado para iterar sobre os dados do endpoint principal\n",
        "                \"fixture\": \"fixture.id\" # O valor do campo \"fixture.id\" do endpoint principal será usado como valor para o parâmetro \"fixture\" da requisição\n",
        "            },\n",
        "            \"fixed_params\": {}, # Parâmetros fixos da requisição para a API-Sports\n",
        "            \"fields\": [\"fixture\"], # Campos a serem selecionados na resposta da API\n",
        "            \"nested_fields\": [ # Campos aninhados a serem extraídos da resposta da API\n",
        "                \"team.id\",\n",
        "                \"team.name\",\n",
        "                \"team.logo\"\n",
        "            ],\n",
        "            \"repeatable_fields\": [ # Campos que se repetem na resposta da API\n",
        "                \"statistics\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"table\": \"fixturesLineups\", # Nome da tabela no BigQuery\n",
        "            \"write_disposition\": \"WRITE_APPEND\", # Define o método de escrita no BigQuery (append)\n",
        "            \"path\": \"fixtures/lineups\", # Caminho do endpoint na API-Sports\n",
        "            \"query_param\": { # Define o parâmetro da requisição para a API-Sports que será usado para iterar sobre os dados do endpoint principal\n",
        "                \"fixture\": \"fixture.id\" # O valor do campo \"fixture.id\" do endpoint principal será usado como valor para o parâmetro \"fixture\" da requisição\n",
        "            },\n",
        "            \"fixed_params\": {}, # Parâmetros fixos da requisição para a API-Sports\n",
        "            \"fields\": [ # Campos a serem selecionados na resposta da API\n",
        "                \"fixture\",\n",
        "                \"formation\"\n",
        "            ],\n",
        "            \"nested_fields\": [ # Campos aninhados a serem extraídos da resposta da API\n",
        "                \"team.id\"\n",
        "            ],\n",
        "            \"repeatable_fields\": [ # Campos que se repetem na resposta da API\n",
        "                \"startXI\"\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t8XXpXNBb7U"
      },
      "outputs": [],
      "source": [
        "# Função para buscar os dados da API\n",
        "def fetch_data(path, params, headers):\n",
        "    \"\"\"\n",
        "    Busca dados da API em várias páginas.\n",
        "\n",
        "    Args:\n",
        "        path (str): O caminho do endpoint da API.\n",
        "        params (dict): Os parâmetros da consulta da API.\n",
        "        headers (dict): Os cabeçalhos da requisição da API.\n",
        "\n",
        "    Returns:\n",
        "        list: Uma lista de dicionários contendo os dados da API.\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "    while True:\n",
        "        response = requests.get(f\"{API_URL}/{path}\", headers=headers, params=params)\n",
        "        data = response.json()\n",
        "        print(response.text)\n",
        "        if 'response' in data and data['response']:\n",
        "            all_data.extend(data['response'])\n",
        "            if 'page' in params:\n",
        "                params['page'] += 1\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "    return all_data\n",
        "\n",
        "\n",
        "# Função para buscar dados de endpoints iteráveis\n",
        "def fetch_iterable_data(main_data, iterable_endpoint):\n",
        "    \"\"\"\n",
        "    Busca dados de um endpoint iterável para cada item em um DataFrame principal.\n",
        "\n",
        "    Args:\n",
        "        main_data (pd.DataFrame): O DataFrame principal contendo dados de um endpoint anterior.\n",
        "        iterable_endpoint (dict): As configurações do endpoint iterável.\n",
        "\n",
        "    Returns:\n",
        "        list: Uma lista de dicionários contendo os dados do endpoint iterável para cada item no DataFrame principal.\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "    for _, item in main_data.iterrows():\n",
        "        query_params = {key: item[value.replace('.', '__')] for key, value in iterable_endpoint[\"query_param\"].items()}\n",
        "        params = query_params.copy()\n",
        "        params.update(iterable_endpoint['fixed_params'])\n",
        "        data = fetch_data(iterable_endpoint['path'], params, HEADERS)\n",
        "\n",
        "        iterated_data = [{**query_params, **item} for item in data]\n",
        "        all_data.extend(iterated_data)\n",
        "    return all_data\n",
        "\n",
        "\n",
        "# Função para preparar o DataFrame\n",
        "def prepare_dataframe(data, fields, nested_fields, repeatable_fields):\n",
        "    \"\"\"\n",
        "    Prepara um DataFrame a partir de uma lista de dicionários, extraindo campos aninhados e repetíveis.\n",
        "\n",
        "    Args:\n",
        "        data (list): A lista de dicionários contendo os dados.\n",
        "        fields (list): Uma lista de nomes de campos a serem incluídos no DataFrame.\n",
        "        nested_fields (list): Uma lista de nomes de campos aninhados a serem extraídos.\n",
        "        repeatable_fields (list): Uma lista de nomes de campos repetíveis a serem extraídos.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: O DataFrame preparado.\n",
        "    \"\"\"\n",
        "    for item in data:\n",
        "        for field in repeatable_fields:\n",
        "            if field in item:\n",
        "                for sub_item in item[field]:\n",
        "                    for key in sub_item.keys():\n",
        "                        sub_item[key] = str(sub_item[key])\n",
        "\n",
        "    meta_fields = fields + repeatable_fields\n",
        "\n",
        "    df = pd.json_normalize(data, sep='__', meta=meta_fields)\n",
        "\n",
        "    all_fields = fields + nested_fields + repeatable_fields\n",
        "    column_names = [col.replace('.', '__') for col in all_fields]\n",
        "\n",
        "    existing_columns = [col for col in column_names if col in df.columns]\n",
        "\n",
        "    df = df[existing_columns]\n",
        "    return df\n",
        "\n",
        "\n",
        "# Função para criar um conjunto de dados no BigQuery caso ele não exista\n",
        "def create_dataset_if_not_exists(client, dataset_name):\n",
        "    \"\"\"\n",
        "    Cria um conjunto de dados no BigQuery se ele não existir.\n",
        "\n",
        "    Args:\n",
        "        client: O cliente do BigQuery.\n",
        "        dataset_name (str): O nome do conjunto de dados.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        client.get_dataset(dataset_name)\n",
        "    except NotFound:\n",
        "        print(f\"Dataset {dataset_name} not found. Creating...\")\n",
        "        client.create_dataset(dataset_name)\n",
        "\n",
        "\n",
        "# Função para obter o esquema de uma tabela no BigQuery\n",
        "def get_table_schema(client, dataset_name, table_name):\n",
        "    \"\"\"\n",
        "    Obtém o esquema de uma tabela no BigQuery.\n",
        "\n",
        "    Args:\n",
        "        client: O cliente do BigQuery.\n",
        "        dataset_name (str): O nome do conjunto de dados.\n",
        "        table_name (str): O nome da tabela.\n",
        "\n",
        "    Returns:\n",
        "        google.cloud.bigquery.schema.SchemaField: O esquema da tabela.\n",
        "    \"\"\"\n",
        "    table_ref = client.dataset(dataset_name).table(table_name)\n",
        "    table = client.get_table(table_ref)\n",
        "    return table.schema\n",
        "\n",
        "\n",
        "# Função para carregar dados no BigQuery\n",
        "def load_data_to_bigquery (client, dataset_name, table_name, data, write_disposition, partition_column=None, clustering_fields=None):\n",
        "   \n",
        "    table_ref = client.dataset(dataset_name).table(table_name)\n",
        "\n",
        "    try:\n",
        "        table = client.get_table(table_ref)\n",
        "        schema = table.schema\n",
        "    except NotFound:\n",
        "        schema = None\n",
        "        print(f\"Schema para a tabela {DATASET_NAME}.{table_name} não encontrada.\")\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=write_disposition,\n",
        "    )\n",
        "    if schema:\n",
        "        job_config.schema = schema\n",
        "    else:\n",
        "        job_config.autodetect = True\n",
        "\n",
        "    json_str = data.to_json(orient='records', date_format='iso')\n",
        "    job = client.load_table_from_json(json.loads(json_str), table_ref, job_config=job_config)\n",
        "    job.result()\n",
        "    print(f\"Foram carregadas {len(data)} linhas em {dataset_name}.{table_name}\")\n",
        "\n",
        "\n",
        "# Função para obter a última data de atualização da tabela de controle\n",
        "def get_last_update(client, now):\n",
        "    \"\"\"\n",
        "    Obtém a última data de atualização da tabela de controle.\n",
        "\n",
        "    Args:\n",
        "        client: O cliente do BigQuery.\n",
        "        now (datetime): A data e hora atuais.\n",
        "\n",
        "    Returns:\n",
        "        datetime: A última data de atualização.\n",
        "    \"\"\"\n",
        "    table_name = f'{PROJECT_ID}.{DATASET_NAME}.updates'\n",
        "\n",
        "    try:\n",
        "        client.get_table(table_name)\n",
        "    except NotFound:\n",
        "        print(f\"Table {table_name} not found. Initializing with FULL_LOAD_DATE: {FULL_LOAD_DATE}\")\n",
        "        full_load_date = datetime.strptime(FULL_LOAD_DATE, '%Y-%m-%d')\n",
        "        initial_data = [{'updated_at': full_load_date}]\n",
        "        load_data_to_bigquery(client, DATASET_NAME, 'updates', pd.DataFrame(initial_data), 'WRITE_TRUNCATE')\n",
        "        return full_load_date\n",
        "\n",
        "    query = f'SELECT MAX(updated_at) AS last_update FROM `{table_name}`'\n",
        "\n",
        "    try:\n",
        "        query_job = client.query(query)\n",
        "        results = query_job.result()\n",
        "\n",
        "        for row in results:\n",
        "            print(f\"Ultima data de atualização: {row.last_update}\")\n",
        "            return row.last_update if row.last_update else now - timedelta(days=1)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while executing the query: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Função para registrar a data e hora da atualização\n",
        "def log_update(client, now):\n",
        "    \"\"\"\n",
        "    Registra a data e hora da atualização na tabela de controle.\n",
        "\n",
        "    Args:\n",
        "        client: O cliente do BigQuery.\n",
        "        now (datetime): A data e hora atuais.\n",
        "    \"\"\"\n",
        "    table_name = f'{PROJECT_ID}.{DATASET_NAME}.updates'\n",
        "    updated_at = [{'updated_at': now}]\n",
        "    load_data_to_bigquery(client, DATASET_NAME, 'updates', pd.DataFrame(updated_at), 'WRITE_APPEND')\n",
        "\n",
        "\n",
        "# Função para atualizar os parâmetros de data para a carga incremental\n",
        "def incremental_params_update(table, incremental_load_params, params, last_update, now, start_date=None, end_date=None):\n",
        "    \"\"\"\n",
        "    Atualiza os parâmetros de data para a carga incremental.\n",
        "\n",
        "    Args:\n",
        "        table (str): O nome da tabela.\n",
        "        incremental_load_params (dict): O dicionário de parâmetros de carga incremental.\n",
        "        params (dict): O dicionário de parâmetros da API.\n",
        "        last_update (datetime): A última data de atualização.\n",
        "        now (datetime): A data e hora atuais.\n",
        "        start_date (str, optional): A data inicial da carga incremental.\n",
        "        end_date (str, optional): A data final da carga incremental.\n",
        "\n",
        "    Returns:\n",
        "        dict: O dicionário de parâmetros da API atualizado.\n",
        "    \"\"\"\n",
        "    if start_date:\n",
        "        start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    if end_date:\n",
        "        end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "\n",
        "    if table == \"past_fixtures\":\n",
        "        params.update({\n",
        "            'from': (start_date if start_date else last_update).strftime('%Y-%m-%d'),\n",
        "            'to': (end_date if end_date else (now - timedelta(days=1))).strftime('%Y-%m-%d')\n",
        "        })\n",
        "    elif table == \"future_fixtures\":\n",
        "        params.update({\n",
        "            'from': (start_date if start_date else now).strftime('%Y-%m-%d')\n",
        "        })\n",
        "    print(f\"Updated params for {table}: {params}\")\n",
        "    return params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xmx6hEnQEKy3"
      },
      "outputs": [],
      "source": [
        "# Função principal que orquestra a extração, transformação e carregamento dos dados\n",
        "def main(request=None, start_date=None, end_date=None):\n",
        "    \"\"\"\n",
        "    Função principal que orquestra a extração, transformação e carregamento dos dados.\n",
        "\n",
        "    Args:\n",
        "        request: A requisição HTTP.\n",
        "        start_date (str, optional): A data inicial da carga incremental.\n",
        "        end_date (str, optional): A data final da carga incremental.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Uma tupla contendo a mensagem de sucesso e o código de status HTTP.\n",
        "    \"\"\"\n",
        "    client = bigquery.Client(project=PROJECT_ID) # Cria um cliente do BigQuery\n",
        "    create_dataset_if_not_exists(client, DATASET_NAME) # Cria o conjunto de dados se ele não existir\n",
        "    now = datetime.now() # Obtém a data e hora atuais\n",
        "    # now = datetime(2024, 6, 1, 0, 0) # Para testes, defina uma data específica\n",
        "    last_update = get_last_update(client, now) # Obtém a última data de atualização\n",
        "    main_endpoints = endpoints.copy() # Faz uma cópia da lista de endpoints principais\n",
        "    main_iterable_endpoints = iterable_endpoints.copy() # Faz uma cópia do dicionário de endpoints iteráveis\n",
        "\n",
        "    for endpoint in main_endpoints: # Itera sobre cada endpoint principal\n",
        "        params = endpoint.get('params', {}) # Obtém os parâmetros do endpoint\n",
        "        table = endpoint.get('table') # Obtém o nome da tabela\n",
        "        incremental_load_params = endpoint.get('incremental_load_params') # Obtém os parâmetros de carga incremental\n",
        "        path = endpoint.get('path') # Obtém o caminho do endpoint\n",
        "        fields = endpoint.get('fields') # Obtém os campos a serem selecionados\n",
        "        nested_fields = endpoint.get('nested_fields') # Obtém os campos aninhados\n",
        "        repeatable_fields = endpoint.get('repeatable_fields') # Obtém os campos repetíveis\n",
        "        write_disposition = endpoint.get('write_disposition') # Obtém a disposição de gravação\n",
        "        quality_control = endpoint.get('quality_control', False) # Obtém a flag de controle de qualidade\n",
        "\n",
        "        if incremental_load_params: # Verifica se há parâmetros de carga incremental\n",
        "            params = incremental_params_update(table, incremental_load_params, params, last_update, now, start_date, end_date) # Atualiza os parâmetros da API com as datas de início e fim da carga incremental\n",
        "\n",
        "        raw_data = fetch_data(path, params, HEADERS) # Busca os dados brutos da API\n",
        "\n",
        "        if raw_data: # Verifica se há dados brutos\n",
        "            prepared_data = prepare_dataframe(raw_data, fields, nested_fields, repeatable_fields) # Prepara o DataFrame\n",
        "            load_data_to_bigquery(client, DATASET_NAME, table, prepared_data, write_disposition) # Carrega os dados no BigQuery\n",
        "\n",
        "            if table in main_iterable_endpoints: # Verifica se há endpoints iteráveis para o endpoint principal atual\n",
        "                for iterable_endpoint in main_iterable_endpoints[table]: # Itera sobre cada endpoint iterável\n",
        "                    iterable_table = iterable_endpoint.get('table') # Obtém o nome da tabela do endpoint iterável\n",
        "                    iterable_fields = iterable_endpoint.get('fields') # Obtém os campos a serem selecionados do endpoint iterável\n",
        "                    iterable_nested_fields = iterable_endpoint.get('nested_fields') # Obtém os campos aninhados do endpoint iterável\n",
        "                    iterable_repeatable_fields = iterable_endpoint.get('repeatable_fields') # Obtém os campos repetíveis do endpoint iterável\n",
        "                    iterable_write_disposition = iterable_endpoint.get('write_disposition') # Obtém a disposição de gravação do endpoint iterável\n",
        "\n",
        "                    print(f\"Buscando os dados iteráveis para o endpoint: {iterable_table}\") # Imprime uma mensagem informando que está buscando dados iteráveis\n",
        "                    detailed_data = fetch_iterable_data(prepared_data, iterable_endpoint) # Busca os dados do endpoint iterável\n",
        "\n",
        "                    if detailed_data: # Verifica se há dados do endpoint iterável\n",
        "                        prepared_iterable_data = prepare_dataframe(detailed_data, iterable_fields, iterable_nested_fields, iterable_repeatable_fields) # Prepara o DataFrame do endpoint iterável\n",
        "                        load_data_to_bigquery(client, DATASET_NAME, iterable_table, prepared_iterable_data, iterable_write_disposition) # Carrega os dados do endpoint iterável no BigQuery\n",
        "                    else:\n",
        "                        print(f\"Não foram encontrados dados para o endpoint: {iterable_table}\") # Imprime uma mensagem se não houver dados do endpoint iterável\n",
        "        else:\n",
        "            print(f\"Não foram encontrados dados para o endpoint: {table}\") # Imprime uma mensagem se não houver dados brutos\n",
        "\n",
        "    log_update(client, now) # Registra a data e hora da atualização\n",
        "\n",
        "    return \"success\", 200 # Retorna uma mensagem de sucesso e o código de status HTTP 200\n",
        "\n",
        "main()\n",
        "#main(start_date=\"2023-01-01\", end_date=\"2024-07-31\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
